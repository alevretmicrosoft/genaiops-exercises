{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Goal of the Exercise  \n",
    "\n",
    "In this exercise, you'll learn how to evaluate the **quality of AI-generated text responses** using advanced, AI-assisted evaluators such as **Relevance**, **Coherence**, **Fluency**, **Groundedness**, and custom-built evaluators. Your tasks involve implementing, configuring, and applying these evaluators to ensure generated content meets high-quality standards.\n",
    "\n",
    "Through this exercise, you'll gain practical experience in:\n",
    "\n",
    "- Identifying the key dimensions of text quality (relevance, coherence, fluency, groundedness).\n",
    "- Leveraging built-in and custom evaluators to objectively assess text outputs.\n",
    "- Creating specialized evaluators tailored to your specific use cases or quality standards.\n",
    "\n",
    "By completing these tasks, you'll acquire essential skills to confidently evaluate and improve the quality of your AI models' outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azure-ai-evaluation\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with RelevanceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f224e40",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Relevance Evaluator\n",
    "Fill in the missing code to initialize and use the `RelevanceEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the RelevanceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with CoherenceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457e717",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Coherence Evaluator\n",
    "Complete the code below to instantiate and use the `CoherenceEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the CoherenceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with FluencyEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9c4de",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Fluency Evaluator\n",
    "Fill in the missing code to initialize and use the `FluencyEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the FluencyEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6b81d",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Groundedness Evaluator\n",
    "Complete the code below to instantiate and use the `GroundednessEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the GroundednessEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-based evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-based evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bec496",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement a Custom Function-based Evaluator\n",
    "Write a function to measure the length of a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run a function-based evaluator\n",
    "# Example \"Custom evaluator function to calculate response length\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-based evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0255f5",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement a Custom Class-based Evaluator\n",
    "Create a class-based evaluator that checks responses for blocked words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run a class-based evaluator\n",
    "# Example Custom class-based evaluator to check for blocked words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt-based evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpfulness evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cca6a",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Helpfulness Evaluator\n",
    "Complete the code below to instantiate and use the `HelpfulnessEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the HelpfulnessEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON accuracy evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c69f4",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement a JSON Schema Evaluator\n",
    "Load a JSON schema and use it to evaluate JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement JSON schema evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf54a19",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Evaluate the dataset with built-in and custom evaluators\n",
    "Fill in the missing code to initialize and different evaluators such as built-in (`RelevanceEvaluator`, `CoherenceEvaluator`, `FluencyEvaluator`, `GroundednessEvaluator`, `RetrievalEvaluator`, etc.) and custom ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the different evaluators. \n",
    "# You can use the model_endpoint module as the target to interact with the model endpoint and get the output to evaluate.\n",
    "# Example: RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator, HelpfulnessEvaluator\n",
    "# You'll need to configure the evaluators with the appropriate column_mapping and run them on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display results dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
